{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Annotation_helper import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets correct values for generated figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set all settings for the resulting figures\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 17\n",
    "BIGGER_SIZE = 19\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "Set paths to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../Data\"\n",
    "structured_data_folder = data_folder + \"/structured_data\"\n",
    "text_path = data_folder + \"/text\"\n",
    "data_path = structured_data_folder + \"/test.csv\"\n",
    "dist_folder = structured_data_folder + \"/Distances\"\n",
    "figure_folder = \"../Figures/Results/Embeddings\"\n",
    "model_folder = \"../Models/Embeddings\"\n",
    "matches = pd.read_csv(structured_data_folder + \"/Matches/Bas.csv\", index_col=0)\n",
    "figure_folder = \"../Figures/Results/Classic/\"\n",
    "error_folder = structured_data_folder + \"/Errors/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transaction and permission datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = load_transactions(structured_data_folder + \"\\VOC Cochin Slave Transactions 1706-1801 - IISH - Database - 2018 - v1.csv\")\n",
    "permissions = load_permissions(structured_data_folder + \"\\VOC Cochin Slave Transport Permissions 1770-1795 - IISH - Database - 2018 - v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load distances between datasets from multiple files and merge them into one dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.read_csv(dist_folder + \"/distances_0.csv\", dtype={'permission_indx': 'uint16', 'transaction_indx': 'uint16', 'SlaafGender': 'bool', 'SlaafNaamNieuw': 'float16', 'BezitterGender': 'bool', 'BezitterVoornaam': 'float16', 'BezitterBeroep': 'float16', 'BezitterAchternaam': 'float16'})\n",
    "for i in range(1, 99):\n",
    "    df2 = pd.read_csv(f\"{dist_folder}/distances_{i}.csv\", dtype={'permission_indx': 'uint16', 'transaction_indx': 'uint16', 'SlaafGender': 'bool', 'SlaafNaamNieuw': 'float16', 'BezitterGender': 'bool', 'BezitterVoornaam': 'float16', 'BezitterBeroep': 'float16', 'BezitterAchternaam': 'float16'})\n",
    "    distances = pd.concat([distances, df2])\n",
    "distances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a test and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = distances[distances['permission_indx'].isin(matches[\"permission_indx\"].unique())]\n",
    "all_data[\"Match\"] = 0\n",
    "for i, match in matches.iterrows():\n",
    "    if match[\"transaction_indx\"] != \"None\":\n",
    "        indx = distances.query(f\"permission_indx == {match['permission_indx']} & transaction_indx == {match['transaction_indx']}\").index\n",
    "        all_data.loc[indx, \"Match\"] = 1\n",
    "data = all_data.iloc[:, :-1]\n",
    "y = all_data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of matches in training set: {y_train.sum()}\")\n",
    "print(f\"Amount of matches in test set: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample1 = all_data[all_data[\"Match\"] == 0].sample(61)\n",
    "# sample2 = all_data[all_data[\"Match\"] == 1].sample(60)\n",
    "\n",
    "# sample = pd.concat([sample1, sample2])\n",
    "# y = sample[\"Match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, model_name=\"Default_model\", ax_labels=('x', 'y'), colors = (\"b\", \"r\"), labels=(\"non-matches\", \"matches\")):\n",
    "    clf = model.fit(X, y)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    # title for the plots\n",
    "    title = (f'Decision surface of {model_name}')\n",
    "    # Set-up grid for plotting.\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    for i in range(0, 2):\n",
    "        mask = (y == i)\n",
    "        tX = X[mask]\n",
    "        ty = y[mask]\n",
    "        X0, X1 = tX[:, 0], tX[:, 1]\n",
    "        ax.scatter(X0, X1, c=colors[i], s=20, edgecolors='k', label=labels[i])\n",
    "    ax.set_ylabel(f'{ax_labels[1]} similarity')\n",
    "    ax.set_xlabel(f'{ax_labels[0]} similarity')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_folder + f\"Decision_boundary_{model_name}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_boundary(X, y, make_pipeline(StandardScaler(), LogisticRegressionCV()), \"Logistic_regression\", (\"slave name\", \"owner first name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_boundary(X, y, make_pipeline(StandardScaler(), AdaBoostClassifier(n_estimators=30, random_state=0)), \"ADAboost\", (\"slave name\", \"owner first name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_boundary(X, y, make_pipeline(StandardScaler(), MLPClassifier(activation='relu', solver='adam', alpha=1e-5, hidden_layer_sizes=(30, 30), learning_rate='adaptive', random_state=1, verbose=False)), \"Neural Network\", (\"slave name\", \"owner first name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_decision_boundary(X, y, KNeighborsClassifier(n_neighbors=1), \"KNN\", (\"slave name\", \"owner first name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking\n",
    "Perform blocking to reduce the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train.shape[0]} rows before blocking\")\n",
    "# mask = (X_train[\"SlaafNaamNieuw\"] > 0.3) & (X_train[\"SlaafGender\"] > 0.1) & (X_train[\"BezitterAchternaam\"] > 0.1) & (X_train[\"BezitterVoornaam\"] > 0.1)\n",
    "mask = (X_train[\"SlaafNaamNieuw\"] > 0.6)\n",
    "\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "print(f\"{X_train.shape[0]} rows after blocking\")\n",
    "\n",
    "X_test_pd = X_test\n",
    "y_test_pd = y_test\n",
    "\n",
    "X_train = X_train.iloc[:,3:].to_numpy().astype(float)\n",
    "y_train = y_train.to_numpy().astype(float)\n",
    "X_test = X_test.iloc[:,3:].to_numpy().astype(float)\n",
    "y_test = y_test.to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data[\"SlaafNaamNieuw\"] > 0.6)\n",
    "X = data[mask]\n",
    "y2 = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule_based_classifier():\n",
    "    def __init__(self, alg):\n",
    "        self.alg = alg\n",
    "        \n",
    "    def predict_proba(self, X_test):\n",
    "        pred = self.alg(X_test)\n",
    "        return pred\n",
    "    \n",
    "    def predict(self, X_test, thresh):\n",
    "        pred = self.alg(X_test)\n",
    "        return (pred >= thresh).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact match\n",
    "The exact match algorithm determines the likelyhood of a match using the amount of features that match exactly between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(X_test):\n",
    "    pred = []\n",
    "    for x in X_test:\n",
    "        pred.append((x == 1.).sum())\n",
    "    return np.array(pred)/X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM = Rule_based_classifier(exact_match)\n",
    "pred = EM.predict(X_test, 0.56)\n",
    "evaluate_prediction(pred, y_test, model_name = \"Exact Match\", figure_folder=figure_folder)\n",
    "Classifiers[\"Exact Match\"] = EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy match\n",
    "The fuzzy match algorithm determines the likelyhood of a match using the amount of features that have a similarity above a given threshold between the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(X_test):\n",
    "    pred = []\n",
    "    for x in X_test:\n",
    "        pred.append((x > 0.8).sum())\n",
    "    return np.array(pred)/X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM = Rule_based_classifier(fuzzy_match)\n",
    "pred = FM.predict(X_test, 0.6)\n",
    "evaluate_prediction(pred, y_test, model_name = \"Fuzzy Match\", figure_folder=figure_folder)\n",
    "Classifiers[\"Fuzzy Match\"] = FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All 0\n",
    "The all 0 method always predicts that 2 features are not a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros(len(y_test))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(pred, y_test, model_name=\"All 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNNClf = fit_and_test_classifier(KNeighborsClassifier(n_neighbors=2), X_train, X_test, y_train, y_test, model_name=\"KNN\", figure_folder=figure_folder)\n",
    "\n",
    "Classifiers[\"KNeigbors classifier\"] = KNNClf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = make_pipeline(StandardScaler(),AdaBoostClassifier(n_estimators=30, random_state=0))\n",
    "ADAboost = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"ADAboost\", figure_folder=figure_folder)\n",
    "\n",
    "Classifiers[\"ADAboost\"] = ADAboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1']\n",
    "clf =  make_pipeline(StandardScaler(), LogisticRegressionCV(scoring='f1', penalty='l2', max_iter=200))\n",
    "scores = cross_validate(clf, X, y2, cv=5, n_jobs=5, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = make_pipeline(StandardScaler(), LogisticRegressionCV(scoring='f1', penalty='l2'))\n",
    "LRClf = fit_and_test_classifier(est, X, X, y2, y2, model_name=\"Log_reg\", figure_folder=figure_folder)\n",
    "\n",
    "Classifiers[\"Logistic Regression\"] = LRClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "est = make_pipeline(StandardScaler(), LogisticRegressionCV(scoring='f1', penalty='l2'))\n",
    "LRClf = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"Log_reg\", figure_folder=figure_folder)\n",
    "\n",
    "Classifiers[\"Logistic Regression\"] = LRClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRClf.steps[1][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# ax = fig.add_axes([0,0,1,1])\n",
    "langs = data.columns\n",
    "langs = [\"Slave gender\", \"Slave new name\", \"Owner gender\", \"Owner first name\", \"Owner second name\", \"Owner last name\", \"Owner occupation\", \"Owner status\", \"Owner Ethnicity\"]\n",
    "students = LRClf.steps[1][1].coef_[0]\n",
    "plt.bar(langs,students)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Coef value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_folder + \"Coefs.jpg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = make_pipeline(Normalizer(), MultinomialNB())\n",
    "NBClf = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"NB\", figure_folder=figure_folder)\n",
    "\n",
    "Classifiers[\"Naive Bayes\"] = NBClf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "est = make_pipeline(StandardScaler(), MLPClassifier(activation='relu', solver='adam', alpha=1e-5, hidden_layer_sizes=(30, 30), learning_rate='adaptive', random_state=1, verbose=False))\n",
    "\n",
    "NNClf = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"MLP\", figure_folder=figure_folder)\n",
    "Classifiers[\"Neural network\"] = NNClf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = make_pipeline(StandardScaler(), LinearSVC(random_state=0))\n",
    "LSVC = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"MLP\", figure_folder=figure_folder)\n",
    "Classifiers[\"Linear SVC\"] = LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix_to_columns(df, prefix):\n",
    "    # Adds a given prefix to all column names\n",
    "    cols = df.columns\n",
    "    df.columns = [prefix + c for c in cols]\n",
    "    return df\n",
    "\n",
    "def get_mistakes(y_pred, y_true, X_array, X_df, t, p):\n",
    "    # Filters out errors in predictions and saves them as csv files\n",
    "    p.set_index(\"Entry-ID\")\n",
    "    mist_indx = np.invert(y_true == y_pred)\n",
    "    mist_dist = X_df[mist_indx]\n",
    "    print(mist_dist.shape)\n",
    "    mist_p = p.loc[mist_dist['permission_indx'], :]\n",
    "    mist_t = t.loc[mist_dist['transaction_indx'], :]\n",
    "    vt = add_prefix_to_columns(mist_t, \"transaction_\")\n",
    "    vp = add_prefix_to_columns(mist_p, 'permission_')\n",
    "    vals = np.concatenate((vt, vp), axis=1)\n",
    "    mist_data = pd.DataFrame(data=vals, columns=list(vt) + list(vp))\n",
    "    mist_data[\"Match\"] = y_test[mist_indx]\n",
    "    mist_data[mist_data[\"Match\"] == 0].to_csv(error_folder + \"False_Positives.csv\")\n",
    "    mist_data[mist_data[\"Match\"] == 1].to_csv(error_folder + \"False_Negatives.csv\")\n",
    "    return mist_data\n",
    "\n",
    "pred = LRClf.predict(X_test)\n",
    "mist = get_mistakes(pred, y_test, X_test, X_test_pd, transactions, permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(LRClf, X_test, y_test, name=\"Logistic regression\", ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf_name in Classifiers:\n",
    "    clf = Classifiers[clf_name]\n",
    "    try:\n",
    "        plot_precision_recall_curve(clf, X_test, y_test, name=clf_name, ax=plt.gca())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_layers(layer_size, n_layers):\n",
    "#     res = []\n",
    "#     for ls in layer_size:\n",
    "#         res += [ls for i in range(0, n_layers)]\n",
    "#     return res\n",
    "        \n",
    "# layers = make_layers([100, 50, 20], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# est = make_pipeline(StandardScaler(), MLPClassifier(activation='relu', solver='adam', alpha=1e-5, hidden_layer_sizes=layers, learning_rate='adaptive', random_state=1, verbose=False))\n",
    "\n",
    "# NNClf = fit_and_test_classifier(est, X_train, X_test, y_train, y_test, model_name=\"MLP\", figure_folder=figure_folder)\n",
    "# Classifiers[\"Neural network\"] = NNClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in Classifiers:\n",
    "#     model = Classifiers[name]\n",
    "# #     print(\"name:\", name)\n",
    "#     # predict probabilities\n",
    "#     lr_probs = model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "# #     print(\"lr_probs\", lr_probs)\n",
    "#     if type(lr_probs[0]) == np.ndarray:\n",
    "#         lr_probs = lr_probs[:, 1]\n",
    "#     # calculate scores\n",
    "#     lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "#     # summarize scores\n",
    "#     print(name + ': ROC AUC=%.3f' % (lr_auc))\n",
    "#     # calculate roc curves\n",
    "#     lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
    "#     # plot the roc curve for the model\n",
    "#     plt.plot(lr_fpr, lr_tpr, marker='.', label=name)\n",
    "    \n",
    "# ns_probs = [0 for _ in range(len(y_test))]\n",
    "# ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "# ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "# plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "# print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# # show the legend\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
